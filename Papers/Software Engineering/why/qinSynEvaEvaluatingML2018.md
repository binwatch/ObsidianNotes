# SynEva: Evaluating ML Programs by Mirror Program Synthesis

## MetaData

* Tags: #machine-learning, #mirror-program, #program-synthesis, #similarity-measurement
* Date: [[7/2018]]
* Authors: [[Yi Qin]], [[Huiyan Wang]], [[Chang Xu]], [[Xiaoxing Ma]], [[Jian Lu]]
* URL: [https://ieeexplore.ieee.org/document/8424969/](https://ieeexplore.ieee.org/document/8424969/)

## Overview

```ad-quote
title: Abstract
## Abstract

Machine learning (ML) programs are being widely used in various human-related applications. However, their testing always remains to be a challenging problem, and one can hardly decide whether and how the existing knowledge extracted from training scenarios suit new scenarios. Existing approaches typically have restricted usages due to their assumptions on the availability of an oracle, comparable implementation, or manual inspection efforts. We solve this problem by proposing a novel program synthesis based approach, SynEva, that can systematically construct an oracle-alike mirror program for similarity measurement, and automatically compare it with the existing knowledge on new scenarios to decide how the knowledge suits the new scenarios. SynEva is lightweight and fully automated. Our experimental evaluation with real-world data sets validates SynEvaâ€™s effectiveness by strong correlation and little overhead results. We expect that SynEva can apply to, and help evaluate, more ML programs for new scenarios.

```

### Background

### Problem

### Method

## Zotero links

* [Local library](zotero://select/items/1_N3CUYJKE)
* PDF Attachments
	- [Qin et al. - 2018 - SynEva Evaluating ML Programs by Mirror Program S.pdf](zotero://open-pdf/library/items/95NQAWQQ)
* DOI: [10.1109/QRS.2018.00031](https://doi.org/10.1109/QRS.2018.00031)
* Cite key: qinSynEvaEvaluatingML2018

## Notes











***

